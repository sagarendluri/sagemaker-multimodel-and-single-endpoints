{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker==1.0.0\n",
      "  Downloading sagemaker-1.0.0.tar.gz (120 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\sagar\\anaconda3\\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\sagar\\\\AppData\\\\Local\\\\Temp\\\\pip-install-blld7fxl\\\\sagemaker\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\sagar\\\\AppData\\\\Local\\\\Temp\\\\pip-install-blld7fxl\\\\sagemaker\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\sagar\\AppData\\Local\\Temp\\pip-install-blld7fxl\\sagemaker\\pip-egg-info'\n",
      "         cwd: C:\\Users\\sagar\\AppData\\Local\\Temp\\pip-install-blld7fxl\\sagemaker\\\n",
      "    Complete output (9 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\sagar\\AppData\\Local\\Temp\\pip-install-blld7fxl\\sagemaker\\setup.py\", line 18, in <module>\n",
      "        long_description=read('README.rst'),\n",
      "      File \"C:\\Users\\sagar\\AppData\\Local\\Temp\\pip-install-blld7fxl\\sagemaker\\setup.py\", line 9, in read\n",
      "        return open(os.path.join(os.path.dirname(__file__), fname)).read()\n",
      "      File \"C:\\Users\\sagar\\anaconda3\\lib\\encodings\\cp1252.py\", line 23, in decode\n",
      "        return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n",
      "    UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 2576: character maps to <undefined>\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "BUCKET      = sagemaker_session.default_bucket()\n",
    "TRAINING_FILE     = 'prediction2.py'\n",
    "INFERENCE_FILE = 'inference.py'\n",
    "SOURCE_DIR = 'source_dir'\n",
    "\n",
    "DATA_PREFIX            = 'RF_SCIKIT'\n",
    "MULTI_MODEL_ARTIFACTS  = 'multi_model_artifacts'\n",
    "\n",
    "TRAIN_INSTANCE_TYPE    = 'ml.m4.xlarge'\n",
    "ENDPOINT_INSTANCE_TYPE = 'ml.m4.xlarge'\n",
    "\n",
    "ENDPOINT_NAME = 'model-random-forest-new'\n",
    "\n",
    "MODEL_NAME = ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 7\n",
    "SPLIT_RATIOS = [0.6, 0.3, 0.1]\n",
    "import boto3\n",
    "def split():\n",
    "    bucket_name = 'sagemaker-data-icr'\n",
    "    s3 = boto3.resource('s3')\n",
    "    data_key = 'first phenomes and genomes.csv'\n",
    "    data_location = 's3://{}/{}'.format(bucket_name, data_key)\n",
    "    df = pd.read_csv(data_location)\n",
    "    #data = df.drop(['Unnamed: 0'],axis=1)\n",
    "    model_data = df.fillna(df.mean())\n",
    "    COLUMNS = list(df.columns)\n",
    "    \n",
    "#     #i = str(input(\"enter a feature name \"))\n",
    "#     X =df1.drop([loc], axis = 1)\n",
    "   \n",
    "#     y =df1[loc]\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "#     #X_train_col = X_train.columns\n",
    "#     return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    #i = str(input(\"enter a feature name \"))\n",
    "    #for i in col:\n",
    "   \n",
    "    # split data into train and test sets\n",
    "    seed      = SEED\n",
    "    val_size  = SPLIT_RATIOS[1]\n",
    "    test_size = SPLIT_RATIOS[2]\n",
    "    \n",
    "    _train, _val, _test = np.split(model_data.sample(frac=1, random_state=123), \n",
    "                                                  [int(0.7 * len(model_data)), int(0.9*len(model_data))]) \n",
    "    #train =  train_data.drop([i], axis=1)#, index=False)\n",
    "    #val =   validation_data.drop([i], axis=1)#, index=False)\n",
    "    #test =  test_data.drop([i], axis=1)#, index=False)\n",
    "\n",
    "# Dropping the target value, as we will use this CSV file for batch transform\n",
    "    #test = test_data.drop([i], axis=1).to_csv('test.csv', index=False, header=False)\n",
    "\n",
    "    return _train, _val, _test ,COLUMNS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save_data_locally(location, train, val, test,COLUMNS):\n",
    "\n",
    "#     header = ','.join(COLUMNS)\n",
    "#     print(header)\n",
    "#     print(train)\n",
    "    os.makedirs(f'data/{location}/train')\n",
    "    np.savetxt(f'data/{location}/train/{location}_train.csv', train, header=','.join(COLUMNS), delimiter=',', fmt='%.10f',comments=\"\")\n",
    "    \n",
    "    os.makedirs(f'data/{location}/val')\n",
    "    np.savetxt(f'data/{location}/val/{location}_val.csv',     val,  header = ','.join(COLUMNS), delimiter=',', fmt='%.10f',comments=\"\")\n",
    "    \n",
    "    os.makedirs(f'data/{location}/test')\n",
    "    np.savetxt(f'data/{location}/test/{location}_test.csv',   test,  header = ','.join(COLUMNS),  delimiter=',', fmt='%.10f',comments=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded: s3://sagemaker-ap-south-1-580246529711/RF_SCIKIT/model_prep/100SDW_EIAR_2008_RF\n",
      "1 training jobs launched: [<sagemaker.sklearn.estimator.SKLearn object at 0x7fb39e83f940>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded: s3://sagemaker-ap-south-1-580246529711/RF_SCIKIT/model_prep/100SDW_EIAR_2009_RF\n",
      "2 training jobs launched: [<sagemaker.sklearn.estimator.SKLearn object at 0x7fb39e83f940>, <sagemaker.sklearn.estimator.SKLearn object at 0x7fb39f5a4e80>]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "def launch_training_job(location):\n",
    "    # clear out old versions of the data\n",
    "    s3_bucket = s3.Bucket(BUCKET)\n",
    "    full_input_prefix = f'{DATA_PREFIX}/model_prep/{location}'\n",
    "    s3_bucket.objects.filter(Prefix=full_input_prefix + '/').delete()\n",
    "\n",
    "    # upload the entire set of data for all three channels\n",
    "    local_folder = f'data/{location}'\n",
    "    inputs = sagemaker_session.upload_data(path=local_folder, \n",
    "                                            key_prefix=full_input_prefix)\n",
    "    \n",
    "    print(f'Training data uploaded: {inputs}')\n",
    "    \n",
    "    _job = 'gp-{}'.format(location.replace('_', '-'))\n",
    "    full_output_prefix = f'{DATA_PREFIX}/model_artifacts/{location}'\n",
    "    s3_output_path = f's3://{BUCKET}/{full_output_prefix}'\n",
    "    \n",
    "    code_location = f's3://{BUCKET}/{full_input_prefix}/code'\n",
    "    # Return the estimator object\n",
    "    return s3_output_path,_job, code_location ,inputs,location \n",
    "\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "_train, _val, _test ,COLUMNS  = split()\n",
    "\n",
    "LOCATIONS  = COLUMNS\n",
    "PARALLEL_TRAINING_JOBS =2\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "estimators = []\n",
    "_train, _val, _test ,COLUMNS  = split()\n",
    "shutil.rmtree('data', ignore_errors=True)\n",
    "for loc in LOCATIONS[:PARALLEL_TRAINING_JOBS]:\n",
    "    _train, _val, _test ,COLUMNS = split()\n",
    "    save_data_locally(loc, _train, _val, _test ,COLUMNS)\n",
    "    \n",
    "    s3_output_path,_job, code_location,inputs,location = launch_training_job(loc)\n",
    "    _estimator = SKLearn(\n",
    "        entry_point=TRAINING_FILE, # script to use for training job\n",
    "        role=role,\n",
    "        source_dir=SOURCE_DIR, # Location of scripts\n",
    "        train_instance_count=1,\n",
    "        train_instance_type=TRAIN_INSTANCE_TYPE,\n",
    "        framework_version='0.23-1',# 0.23-1 is the latest version\n",
    "        output_path=s3_output_path,# Where to store model artifacts\n",
    "        base_job_name=_job,\n",
    "        code_location=code_location,\n",
    "        hyperparameters =  {'model-name': location})\n",
    "    \n",
    "    \n",
    "    DISTRIBUTION_MODE = 'FullyReplicated'\n",
    "    \n",
    "    \n",
    "    train_input = sagemaker.s3_input(s3_data=inputs+'/train', \n",
    "                                      distribution=DISTRIBUTION_MODE, content_type='csv')\n",
    "    \n",
    "    val_input   = sagemaker.s3_input(s3_data=inputs+'/val', \n",
    "                                      distribution=DISTRIBUTION_MODE, content_type='csv')\n",
    "    \n",
    "    remote_inputs = {'train': train_input, 'validation': val_input}\n",
    "  \n",
    "    \n",
    "    #eval_set=[(train_input2, val_input2)]\n",
    "    #eval_set = {'train': train_input2,'test': val_input2}\n",
    "\n",
    "\n",
    "    _estimator.fit(remote_inputs,wait = False)#,eval_set)\n",
    "    estimators.append(_estimator)\n",
    "\n",
    "    \n",
    "    print('{} training jobs launched: {}'.format(len(estimators), estimators))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2 training jobs launched: ['gp-100SDW-EIAR-2008-RF-2020-07-28-15-09-06-203', 'gp-100SDW-EIAR-2009-RF-2020-07-28-15-09-13-556']\n"
     ]
    }
   ],
   "source": [
    "#APB_PAT_2001_RF2\n",
    "#[100SDW_EIAR_2009_RF','100SDW_EU_2008_RF']\n",
    "\n",
    "#2014DSPHY-1nd-layer\n",
    "print()\n",
    "print(f'{len(estimators)} training jobs launched: {[x.latest_training_job.name for x in estimators]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job: gp-100SDW-EIAR-2008-RF-2020-07-28-15-09-06-203\n",
      "gp-100SDW-EIAR-2008-RF-2020-07-28-15-09-06-203 job status: InProgress\n",
      "gp-100SDW-EIAR-2008-RF-2020-07-28-15-09-06-203 job status: InProgress\n",
      "gp-100SDW-EIAR-2008-RF-2020-07-28-15-09-06-203 job status: InProgress\n",
      "gp-100SDW-EIAR-2008-RF-2020-07-28-15-09-06-203 job status: InProgress\n",
      "gp-100SDW-EIAR-2008-RF-2020-07-28-15-09-06-203 job status: InProgress\n",
      "gp-100SDW-EIAR-2008-RF-2020-07-28-15-09-06-203 job status: InProgress\n",
      "gp-100SDW-EIAR-2008-RF-2020-07-28-15-09-06-203 job status: InProgress\n",
      "DONE. Status for gp-100SDW-EIAR-2008-RF-2020-07-28-15-09-06-203 is Failed\n",
      "\n",
      "Waiting for job: gp-100SDW-EIAR-2009-RF-2020-07-28-15-09-13-556\n",
      "DONE. Status for gp-100SDW-EIAR-2009-RF-2020-07-28-15-09-13-556 is Failed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def wait_for_training_job_to_complete(estimator):\n",
    "    job = estimator.latest_training_job.job_name\n",
    "    print(f'Waiting for job: {job}')\n",
    "    status = estimator.latest_training_job.describe()['TrainingJobStatus']\n",
    "    while status == 'InProgress':\n",
    "        time.sleep(45)\n",
    "        status = estimator.latest_training_job.describe()['TrainingJobStatus']\n",
    "        if status == 'InProgress':\n",
    "            print(f'{job} job status: {status}')\n",
    "    print(f'DONE. Status for {job} is {status}\\n')\n",
    "# wait for the jobs to finish\n",
    "for est in estimators:\n",
    "    wait_for_training_job_to_complete(est)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sagemaker'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a8370d64424d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSKLearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gp-100SDW-EIAR-2008-RF-2020-07-28-15-09-06-203'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmy_estimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSKLearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmy_estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sagemaker'"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "job_name='gp-100SDW-EIAR-2008-RF-2020-07-28-15-09-06-203'\n",
    "my_estimator = SKLearn.attach(job_name)\n",
    "my_estimator\n",
    "\n",
    "#aws sagemaker describe-training-job --training-job-name 'GENOME-PHENOME-2020-05-27-13-03-03'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = estimators[0]\n",
    "# inference.py is the entry_point for when we deploy the model\n",
    "# Note how we do NOT specify source_dir again, this information is inherited from the estimator\n",
    "model = estimator.create_model(role=role, entry_point='inference.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "\n",
    "# This is where our MME will read models from on S3.\n",
    "model_data_prefix = f's3://{BUCKET}/{DATA_PREFIX}/{MULTI_MODEL_ARTIFACTS}/'\n",
    "\n",
    "gp = MultiDataModel(name=MODEL_NAME,\n",
    "                     model_data_prefix=model_data_prefix,\n",
    "                     model=model,# passing our model\n",
    "                     sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = gp.deploy(initial_instance_count=1,\n",
    "                       instance_type=ENDPOINT_INSTANCE_TYPE,\n",
    "                       endpoint_name=ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No models visible!\n",
    "list(gp.list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for est in estimators:\n",
    "    artifact_path = est.latest_training_job.describe()['ModelArtifacts']['S3ModelArtifacts']\n",
    "    model_name = artifact_path.split('/')[-4]+'.tar.gz'\n",
    "    # This is copying over the model artifact to the S3 location for the MME.\n",
    "    gp.add_model(model_data_source=artifact_path, model_data_path=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(gp.list_models())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "out_arr = np.random.randint(low = 0, high = 1099, size = 4799)\n",
    "\n",
    "predicted_value = predictor.predict(out_arr, target_model='100SDW_EIAR_2009_RF.tar.gz')\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print('${:,.2f}, took {:,d} ms\\n'.format(predicted_value[0], int(duration * 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "def launch_training_job(i):\n",
    "    \n",
    "\n",
    "    inputs = sagemaker_session.upload_data('data')\n",
    "\n",
    "    print(f'Training data uploaded: {inputs}')\n",
    "\n",
    "    _job = 'gp-{}'.format(i.replace('_', '-'))\n",
    "    inputs = sagemaker_session.upload_data('data')\n",
    "    #print('Training data uploaded: {}'.format(inputs))\n",
    "    \n",
    "    _job = 'gp-{}'.format(i.replace('_', '-'))\n",
    "    _full_output_prefix = '{}/model_artifacts/{}'.format(DATA_PREFIX, \n",
    "                                                        i)\n",
    "    _s3_output_path = 's3://{}/{}'.format(BUCKET, _full_output_prefix)\n",
    "    return _s3_output_path,_job\n",
    "\n",
    "    #return s3_output_path ,code_location ,_job\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "LOCATIONS  = ['100SDW_EIAR_2008_RF','100SDW_EIAR_2009_RF','100SDW_EU_2008_RF','100SDW_EU_2009_RF']\n",
    "PARALLEL_TRAINING_JOBS = 2\n",
    "estimators = []\n",
    "for loc in LOCATIONS[:PARALLEL_TRAINING_JOBS]:\n",
    "    train_data, validation_data, test_data = split() \n",
    "    #df1= train_validate_test_split(df1)\n",
    "    #save_data_locally(loc,df1)\n",
    "   # _s3_output_path,_job = launch_training_job(loc)\n",
    "    # clear out old versions of the data\n",
    "    \n",
    "    _s3_output_path,_job = launch_training_job(i)\n",
    "    _estimator = SKLearn(\n",
    "         entry_point='train_prediction.py', role=role,\n",
    "         train_instance_count=1, train_instance_type=TRAIN_INSTANCE_TYPE,\n",
    "         framework_version='0.20.0',\n",
    "         output_path=_s3_output_path,\n",
    "         base_job_name=_job)\n",
    "    \n",
    "    DISTRIBUTION_MODE = 'FullyReplicated'\n",
    "    \n",
    "    train_input = sagemaker_session.upload_data(\"data\")\n",
    "    _remote_inputs = {'train': train_input}\n",
    "    _estimator.fit(_remote_inputs, wait=False)\n",
    "\n",
    "    #_remote_inputs = {'train': train_input}\n",
    "    #_estimator.fit(_remote_inputs, wait=False)\n",
    "    estimators.append( _estimator)\n",
    "    print('{} training jobs launched: {}'.format(len(estimators), estimators))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APB_PAT_2001_RF2\n",
    "#[100SDW_EIAR_2009_RF','100SDW_EU_2008_RF']\n",
    "\n",
    "#2014DSPHY-1nd-layer\n",
    "print()\n",
    "print(f'{len(estimators)} training jobs launched: {[x.latest_training_job.name for x in estimators]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def wait_for_training_job_to_complete(estimator):\n",
    "    job = estimator.latest_training_job.job_name\n",
    "    print(f'Waiting for job: {job}')\n",
    "    status = estimator.latest_training_job.describe()['TrainingJobStatus']\n",
    "    while status == 'InProgress':\n",
    "        time.sleep(45)\n",
    "        status = estimator.latest_training_job.describe()['TrainingJobStatus']\n",
    "        if status == 'InProgress':\n",
    "            print(f'{job} job status: {status}')\n",
    "    print(f'DONE. Status for {job} is {status}\\n')\n",
    "# wait for the jobs to finish\n",
    "for est in estimators:\n",
    "    wait_for_training_job_to_complete(est)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = estimators[0]\n",
    "# inference.py is the entry_point for when we deploy the model\n",
    "# Note how we do NOT specify source_dir again, this information is inherited from the estimator\n",
    "model = estimator.create_model(role=role)#, entry_point='inference.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "\n",
    "# This is where our MME will read models from on S3.\n",
    "model_data_prefix = f's3://{BUCKET}/{DATA_PREFIX}/{MULTI_MODEL_ARTIFACTS}/'\n",
    "\n",
    "mme = MultiDataModel(name=MODEL_NAME,\n",
    "                     model_data_prefix=model_data_prefix,\n",
    "                     model=model,# passing our model\n",
    "                     sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictor = mme.deploy(initial_instance_count=1,\n",
    "                       instance_type=ENDPOINT_INSTANCE_TYPE,\n",
    "                       endpoint_name='gp-sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "NUM_HOUSES_PER_LOCATION = 1000\n",
    "LOCATIONS  = ['NewYork_NY',    'LosAngeles_CA',   'Chicago_IL',    'Houston_TX',   'Dallas_TX',\n",
    "              'Phoenix_AZ',    'Philadelphia_PA', 'SanAntonio_TX', 'SanDiego_CA',  'SanFrancisco_CA']\n",
    "PARALLEL_TRAINING_JOBS = 2 # len(LOCATIONS) if your account limits can handle it\n",
    "MAX_YEAR = 2019\n",
    "\n",
    "def gen_price(house):\n",
    "    _base_price = int(house['SQUARE_FEET'] * 150)\n",
    "    _price = int(_base_price + (10000 * house['NUM_BEDROOMS']) + \\\n",
    "                               (15000 * house['NUM_BATHROOMS']) + \\\n",
    "                               (15000 * house['LOT_ACRES']) + \\\n",
    "                               (15000 * house['GARAGE_SPACES']) - \\\n",
    "                               (5000 * (MAX_YEAR - house['YEAR_BUILT'])))\n",
    "    return _price\n",
    "\n",
    "def gen_random_house():\n",
    "    _house = {'SQUARE_FEET':   int(np.random.normal(3000, 750)),\n",
    "              'NUM_BEDROOMS':  np.random.randint(2, 7),\n",
    "              'NUM_BATHROOMS': np.random.randint(2, 7) / 2,\n",
    "              'LOT_ACRES':     round(np.random.normal(1.0, 0.25), 2),\n",
    "              'GARAGE_SPACES': np.random.randint(0, 4),\n",
    "              'YEAR_BUILT':    min(MAX_YEAR, int(np.random.normal(1995, 10)))}\n",
    "    _price = gen_price(_house)\n",
    "    return [_price, _house['YEAR_BUILT'],   _house['SQUARE_FEET'], \n",
    "                    _house['NUM_BEDROOMS'], _house['NUM_BATHROOMS'], \n",
    "                    _house['LOT_ACRES'],    _house['GARAGE_SPACES']]\n",
    "\n",
    "def gen_houses(num_houses):\n",
    "    _house_list = []\n",
    "    for i in range(num_houses):\n",
    "        _house_list.append(gen_random_house())\n",
    "    _df = pd.DataFrame(_house_list, \n",
    "                       columns=['PRICE', 'YEAR_BUILT',\n",
    "                                'SQUARE_FEET', 'NUM_BEDROOMS',\n",
    "                                'NUM_BATHROOMS', 'LOT_ACRES',\n",
    "                                'GARAGE_SPACES'])\n",
    "    return _df\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "BUCKET      = sagemaker_session.default_bucket()\n",
    "TRAINING_FILE     = 'training.py'\n",
    "INFERENCE_FILE = 'inference.py'\n",
    "SOURCE_DIR = 'source_dir'\n",
    "\n",
    "DATA_PREFIX            = 'DEMO_MME_SCIKIT_V1'\n",
    "MULTI_MODEL_ARTIFACTS  = 'multi_model_artifacts'\n",
    "\n",
    "TRAIN_INSTANCE_TYPE    = 'ml.m4.xlarge'\n",
    "ENDPOINT_INSTANCE_TYPE = 'ml.m4.xlarge'\n",
    "\n",
    "ENDPOINT_NAME = 'housing-V1'\n",
    "\n",
    "MODEL_NAME = ENDPOINT_NAME\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "SEED = 7\n",
    "SPLIT_RATIOS = [0.6, 0.3, 0.1]\n",
    "\n",
    "def split_data(df):\n",
    "    # split data into train and test sets\n",
    "    seed      = SEED\n",
    "    val_size  = SPLIT_RATIOS[1]\n",
    "    test_size = SPLIT_RATIOS[2]\n",
    "    \n",
    "    num_samples = df.shape[0]\n",
    "    X1 = df.values[:num_samples, 1:] # keep only the features, skip the target, all rows\n",
    "    Y1 = df.values[:num_samples, :1] # keep only the target, all rows\n",
    "\n",
    "    # Use split ratios to divide up into train/val/test\n",
    "    X_train, X_val, y_train, y_val = \\\n",
    "        train_test_split(X1, Y1, test_size=(test_size + val_size), random_state=seed)\n",
    "    # Of the remaining non-training samples, give proper ratio to validation and to test\n",
    "    X_test, X_test, y_test, y_test = \\\n",
    "        train_test_split(X_val, y_val, test_size=(test_size / (test_size + val_size)), \n",
    "                         random_state=seed)\n",
    "    # reassemble the datasets with target in first column and features after that\n",
    "    _train = np.concatenate([y_train, X_train], axis=1)\n",
    "    _val   = np.concatenate([y_val,   X_val],   axis=1)\n",
    "    _test  = np.concatenate([y_test,  X_test],  axis=1)\n",
    "\n",
    "    return _train, _val, _test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "def launch_training_job(location):\n",
    "    # clear out old versions of the data\n",
    "    s3_bucket = s3.Bucket(BUCKET)\n",
    "    full_input_prefix = f'{DATA_PREFIX}/model_prep/{location}'\n",
    "    s3_bucket.objects.filter(Prefix=full_input_prefix + '/').delete()\n",
    "\n",
    "    # upload the entire set of data for all three channels\n",
    "    local_folder = f'data/{location}'\n",
    "    inputs = sagemaker_session.upload_data(path=local_folder, \n",
    "                                            key_prefix=full_input_prefix)\n",
    "    \n",
    "    print(f'Training data uploaded: {inputs}')\n",
    "    \n",
    "    _job = 'skl-{}'.format(location.replace('_', '-'))\n",
    "    full_output_prefix = f'{DATA_PREFIX}/model_artifacts/{location}'\n",
    "    s3_output_path = f's3://{BUCKET}/{full_output_prefix}'\n",
    "    \n",
    "    code_location = f's3://{BUCKET}/{full_input_prefix}/code'\n",
    "    \n",
    "\n",
    "    # Add code_location argument in order to ensure that code_artifacts are stored in the same place.\n",
    "    estimator = SKLearn(\n",
    "        entry_point=TRAINING_FILE, # script to use for training job\n",
    "        role=role,\n",
    "        source_dir=SOURCE_DIR, # Location of scripts\n",
    "        train_instance_count=1,\n",
    "        train_instance_type=TRAIN_INSTANCE_TYPE,\n",
    "        framework_version='0.23-1',# 0.23-1 is the latest version\n",
    "        output_path=s3_output_path,# Where to store model artifacts\n",
    "        base_job_name=_job,\n",
    "        code_location=code_location,# This is where the .tar.gz of the source_dir will be stored\n",
    "        metric_definitions=[\n",
    "            {'Name' : 'median-AE',\n",
    "             'Regex': 'AE-at-50th-percentile: ([0-9.]+).*$'}],\n",
    "        hyperparameters = {'n-estimators'    : 100,\n",
    "                            'min-samples-leaf': 3,\n",
    "                            'model-name'      : location})\n",
    "    \n",
    "    DISTRIBUTION_MODE = 'FullyReplicated'\n",
    "    \n",
    "    train_input = sagemaker.s3_input(s3_data=inputs+'/train', \n",
    "                                      distribution=DISTRIBUTION_MODE, content_type='csv')\n",
    "    \n",
    "    val_input   = sagemaker.s3_input(s3_data=inputs+'/val', \n",
    "                                      distribution=DISTRIBUTION_MODE, content_type='csv')\n",
    "    \n",
    "    remote_inputs = {'train': train_input, 'validation': val_input}\n",
    "\n",
    "    estimator.fit(remote_inputs, wait=False)\n",
    "    \n",
    "    # Return the estimator object\n",
    "    return estimator\n",
    "\n",
    "def save_data_locally(location, train, val, test):\n",
    "#     _header = ','.join(COLUMNS)\n",
    "    \n",
    "    os.makedirs(f'data/{location}/train')\n",
    "    np.savetxt(f'data/{location}/train/{location}_train.csv', train, delimiter=',', fmt='%.2f')\n",
    "    \n",
    "    os.makedirs(f'data/{location}/val')\n",
    "    np.savetxt(f'data/{location}/val/{location}_val.csv',     val,   delimiter=',', fmt='%.2f')\n",
    "    \n",
    "    os.makedirs(f'data/{location}/test')\n",
    "    np.savetxt(f'data/{location}/test/{location}_test.csv',   test,  delimiter=',', fmt='%.2f')\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "estimators = []\n",
    "\n",
    "shutil.rmtree('data', ignore_errors=True)\n",
    "\n",
    "for loc in LOCATIONS[:PARALLEL_TRAINING_JOBS]:\n",
    "    _houses = gen_houses(NUM_HOUSES_PER_LOCATION)\n",
    "    _train, _val, _test = split_data(_houses)\n",
    "    save_data_locally(loc, _train, _val, _test)\n",
    "    estimator = launch_training_job(loc)\n",
    "    estimators.append(estimator)\n",
    "    time.sleep(2) # to avoid throttling the CreateTrainingJob API\n",
    "\n",
    "print()\n",
    "print(f'{len(estimators)} training jobs launched: {[x.latest_training_job.job_name for x in estimators]}')\n",
    "\n",
    "def wait_for_training_job_to_complete(estimator):\n",
    "    job = estimator.latest_training_job.job_name\n",
    "    print(f'Waiting for job: {job}')\n",
    "    status = estimator.latest_training_job.describe()['TrainingJobStatus']\n",
    "    while status == 'InProgress':\n",
    "        time.sleep(45)\n",
    "        status = estimator.latest_training_job.describe()['TrainingJobStatus']\n",
    "        if status == 'InProgress':\n",
    "            print(f'{job} job status: {status}')\n",
    "    print(f'DONE. Status for {job} is {status}\\n')\n",
    "\n",
    "# wait for the jobs to finish\n",
    "for est in estimators:\n",
    "    wait_for_training_job_to_complete(est)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "job_name=\"gp-100SDW-EIAR-2008-RF-2020-07-21-08-41-59-466\"\n",
    "my_estimator = SKLearn.attach(job_name)\n",
    "my_estimator\n",
    "\n",
    "#aws sagemaker describe-training-job --training-job-name 'GENOME-PHENOME-2020-05-27-13-03-03'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[100SDW_EIAR_2009_RF','100SDW_EU_2008_RF']\n",
    "\n",
    "\n",
    "_houses = gen_houses(1000)\n",
    "\n",
    "num_samples = _houses.shape[0]\n",
    "\n",
    "num_samples\n",
    "#_train, _val, _test = split_data(_houses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def gen_hous():\n",
    "    bucket_name = 'sagemaker-data-icr'\n",
    "    s3 = boto3.resource('s3')\n",
    "    data_key = 'first phenomes and genomes.csv'\n",
    "    data_location = 's3://{}/{}'.format(bucket_name, data_key)\n",
    "    df = pd.read_csv(data_location)\n",
    "    #data = df.drop(['Unnamed: 0'],axis=1)\n",
    "    df1 = df.fillna(df.mean())\n",
    "    COLUMNS = list(df1.columns)\n",
    "    \n",
    "    i = str(input(\"enter a feature name \"))\n",
    "    #for i in col:\n",
    "    return i,df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
