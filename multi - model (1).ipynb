{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHM_NAME = 'multi-modelrf-sagar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=multi-model-random-forest-new\n",
    "\n",
    "cd container\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "#region=$(aws configure get region)\n",
    "region=${region:-ap-south-1}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "# aws sagemaker list-models\n",
    "\n",
    "#docker image ls\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}#${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Train multiple house value prediction models\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer\n",
    "import boto3\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_hous():\n",
    "    bucket_name = 'sagemaker-data-icr'\n",
    "    s3 = boto3.resource('s3')\n",
    "    data_key = 'first phenomes and genomes.csv'\n",
    "    data_location = 's3://{}/{}'.format(bucket_name, data_key)\n",
    "    df = pd.read_csv(data_location)\n",
    "    #data = df.drop(['Unnamed: 0'],axis=1)\n",
    "    df1 = df.fillna(df.mean())\n",
    "    COLUMNS = list(df1.columns)\n",
    "    \n",
    "    i = str(input(\"enter a feature name \"))\n",
    "    #for i in col:\n",
    "    return i,df1\n",
    "\n",
    "\n",
    "\n",
    "sm_client = boto3.client(service_name='sagemaker')\n",
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime')\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "ACCOUNT_ID  = boto3.client('sts').get_caller_identity()['Account']\n",
    "REGION      = boto3.Session().region_name\n",
    "BUCKET      = sagemaker_session.default_bucket()\n",
    "SCRIPT_FILENAME     = 'train_prediction.py'\n",
    "USER_CODE_ARTIFACTS = 'gp_code2.tar.gz'\n",
    "\n",
    "\n",
    "\n",
    "DATA_PREFIX            = 'gp_rf2'\n",
    "END_MODEL_NAME     = 'MXNET-endpoints'\n",
    "MULTI_MODEL_ARTIFACTS  = 'multi_model_artifacts'\n",
    "\n",
    "TRAIN_INSTANCE_TYPE    = 'ml.m4.xlarge'\n",
    "ENDPOINT_INSTANCE_TYPE = 'ml.m5.xlarge'\n",
    "#Split a given dataset into train, validation, and test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "def launch_training_job(i):\n",
    "    # clear out old versions of the data\n",
    "   # _s3_bucket = s3.Bucket(BUCKET)\n",
    "   # _full_input_prefix = '{}/model_prep/{}'.format(DATA_PREFIX, location)\n",
    "   # _s3_bucket.objects.filter(Prefix=_full_input_prefix + '/').delete()\n",
    "\n",
    "    # upload the entire set of data for all three channels\n",
    "    #_local_folder = 'data/{}'.format(location)\n",
    "    inputs = sagemaker_session.upload_data('data')\n",
    "    print('Training data uploaded: {}'.format(inputs))\n",
    "    \n",
    "    _job = 'gp-{}'.format(i.replace('_', '-'))\n",
    "    _full_output_prefix = '{}/model_artifacts/{}'.format(DATA_PREFIX, \n",
    "                                                        i)\n",
    "    _s3_output_path = 's3://{}/{}'.format(BUCKET, _full_output_prefix)\n",
    "    return _s3_output_path,_job\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "LOCATIONS  = ['100SDW_EIAR_2008_RF','100SDW_EIAR_2009_RF','100SDW_EU_2008_RF','100SDW_EU_2009_RF']\n",
    "PARALLEL_TRAINING_JOBS = 1\n",
    "training_jobs = []\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "for loc in LOCATIONS[:PARALLEL_TRAINING_JOBS]:\n",
    "    i ,df1=gen_hous() \n",
    "    #df1= train_validate_test_split(df1)\n",
    "    #save_data_locally(loc,df1)\n",
    "   # _s3_output_path,_job = launch_training_job(loc)\n",
    "    # clear out old versions of the data\n",
    "    \n",
    "    _s3_output_path,_job = launch_training_job(i)\n",
    "    _estimator = SKLearn(\n",
    "         entry_point='train_prediction.py', role=role,\n",
    "         train_instance_count=1, train_instance_type=TRAIN_INSTANCE_TYPE,\n",
    "         framework_version='0.20.0',\n",
    "         output_path=_s3_output_path,\n",
    "         base_job_name=_job)\n",
    "    \n",
    "    DISTRIBUTION_MODE = 'FullyReplicated'\n",
    "    \n",
    "    train_input = sagemaker_session.upload_data(\"data\")\n",
    "    _remote_inputs = {'train': train_input}\n",
    "\n",
    "    _estimator.fit(_remote_inputs, wait=False)\n",
    "    training_jobs.append( _estimator.latest_training_job.name)\n",
    "print('{} training jobs launched: {}'.format(len(training_jobs), training_jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_training_job_to_complete(job_name):\n",
    "    print('Waiting for job {} to complete...'.format(job_name))\n",
    "    _resp   = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "    _status = _resp['TrainingJobStatus']\n",
    "    while _status=='InProgress':\n",
    "        time.sleep(60)\n",
    "        _resp   = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "        _status = _resp['TrainingJobStatus']\n",
    "        if _status == 'InProgress':\n",
    "            print('{} job status: {}'.format(job_name, _status))\n",
    "    print('DONE. Status for {} is {}\\n'.format(job_name, _status))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_model_artifacts(model_data_url):\n",
    "    # extract the s3 key from the full url to the model artifacts\n",
    "    _s3_key = model_data_url.split('s3://{}/'.format(BUCKET))[1]\n",
    "    # get the part of the key that identifies the model within the model artifacts folder\n",
    "    _model_name_plus = _s3_key[_s3_key.find('model_artifacts') + len('model_artifacts') + 1:]\n",
    "    # finally, get the unique model name (e.g., \"NewYork_NY\")\n",
    "    _model_name = re.findall('^(.*?)/', _model_name_plus)[0]\n",
    "    return _s3_key, _model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make a copy of the model artifacts from the original output of the training job to the place in\n",
    "# s3 where the multi model endpoint will dynamically load individual models\n",
    "def deploy_artifacts_to_gp(job_name):\n",
    "    _resp = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "    _source_s3_key, _model_name = parse_model_artifacts(_resp['ModelArtifacts']['S3ModelArtifacts'])\n",
    "    _copy_source = {'Bucket': BUCKET, 'Key': _source_s3_key}\n",
    "    _key = '{}/{}/{}.tar.gz'.format(DATA_PREFIX, MULTI_MODEL_ARTIFACTS, _model_name)\n",
    "    \n",
    "    print('Copying {} model\\n   from: {}\\n     to: {}...'.format(_model_name, _source_s3_key, _key))\n",
    "    s3_client.copy_object(Bucket=BUCKET, CopySource=_copy_source, Key=_key)\n",
    "    return _key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job gp-APB-PAT-2001-RF2-2020-06-02-06-03-48-391 to complete...\n",
      "gp-APB-PAT-2001-RF2-2020-06-02-06-03-48-391 job status: InProgress\n",
      "gp-APB-PAT-2001-RF2-2020-06-02-06-03-48-391 job status: InProgress\n",
      "gp-APB-PAT-2001-RF2-2020-06-02-06-03-48-391 job status: InProgress\n",
      "DONE. Status for gp-APB-PAT-2001-RF2-2020-06-02-06-03-48-391 is Completed\n",
      "\n",
      "Copying APB_PAT_2001_RF2 model\n",
      "   from: gp_rf2/model_artifacts/APB_PAT_2001_RF2/gp-APB-PAT-2001-RF2-2020-06-02-06-03-48-391/output/model.tar.gz\n",
      "     to: gp_rf2/multi_model_artifacts/APB_PAT_2001_RF2.tar.gz...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# copy every model except the first one\n",
    "for job in training_jobs:\n",
    "    wait_for_training_job_to_complete(job)\n",
    "    deploy_artifacts_to_gp(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# When using multi-model endpoints with the Scikit Learn container, we need to provide an entry point for\n",
    "# inference that will at least load the saved model. This function uploads a model artifact containing such a\n",
    "# script. This tar.gz file will be fed to the SageMaker multi-model creation and pointed to by the \n",
    "# SAGEMAKER_SUBMIT_DIRECTORY environment variable.\n",
    "\n",
    "def upload_inference_code(script_file_name, prefix):\n",
    "    _tmp_folder = 'inference-code'\n",
    "    if not os.path.exists(_tmp_folder):\n",
    "        os.makedirs(_tmp_folder)\n",
    "    !tar -czvf $_tmp_folder/$USER_CODE_ARTIFACTS $script_file_name > /dev/null\n",
    "    _loc = sagemaker_session.upload_data(_tmp_folder, \n",
    "                                         key_prefix='{}/{}'.format(prefix, _tmp_folder))\n",
    "    return _loc + '/' + USER_CODE_ARTIFACTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_model_entity(multi_model_name, role):\n",
    "    # establish the place in S3 from which the endpoint will pull individual models\n",
    "    _model_url  = 's3://{}/{}/{}/'.format(BUCKET, DATA_PREFIX, MULTI_MODEL_ARTIFACTS)\n",
    "    _container = {\n",
    "        'Image':        MULTI_MODEL_SKLEARN_IMAGE,\n",
    "        'ModelDataUrl': _model_url,\n",
    "        'Mode':         'MultiModel',\n",
    "        'Environment': {\n",
    "            'SAGEMAKER_PROGRAM' : SCRIPT_FILENAME,\n",
    "            'SAGEMAKER_SUBMIT_DIRECTORY' : upload_inference_code(SCRIPT_FILENAME, DATA_PREFIX)\n",
    "        }\n",
    "    }\n",
    "    create_model_response = sm_client.create_model(\n",
    "        ModelName = multi_model_name,\n",
    "        ExecutionRoleArn = role,\n",
    "        Containers = [_container])\n",
    "    \n",
    "    return _model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi model name: MXNET-endpoints-2020-06-08-05-07-42\n",
      "Here are the models that the endpoint has at its disposal:\n",
      "2020-06-02 06:02:52   33.7 KiB 100SDW_PAT_2001_RF.tar.gz\n",
      "2020-06-02 06:07:55   32.0 KiB APB_PAT_2001_RF2.tar.gz\n",
      "\n",
      "Total Objects: 2\n",
      "   Total Size: 65.7 KiB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "multi_model_name = '{}-{}'.format(HOUSING_MODEL_NAME, strftime('%Y-%m-%d-%H-%M-%S', gmtime()))\n",
    "model_url = create_multi_model_entity(multi_model_name, role)\n",
    "print('Multi model name: {}'.format(multi_model_name))\n",
    "\n",
    "print('Here are the models that the endpoint has at its disposal:')\n",
    "!aws s3 ls --human-readable --summarize $model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint config name: MXNET-endpoints-2020-06-08-05-07-42\n",
      "Endpoint name: MXNET-endpoints-2020-06-08-05-07-42\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = multi_model_name\n",
    "print('Endpoint config name: ' + endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': 'ml.m5.xlarge',\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'ModelName'   : multi_model_name,\n",
    "        'VariantName' : 'AllTraffic'}])\n",
    "\n",
    "endpoint_name = multi_model_name\n",
    "print('Endpoint name: ' + endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint config name: MXNET-endpoints-2020-06-08-05-07-42\n",
      "Endpoint name: MXNET-endpoints-2020-06-08-05-07-42\n",
      "Endpoint Arn: arn:aws:sagemaker:ap-south-1:580246529711:endpoint/mxnet-endpoints-2020-06-08-05-07-42\n",
      "Waiting for MXNET-endpoints-2020-06-08-05-07-42 endpoint to be in service...\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = multi_model_name\n",
    "print('Endpoint config name: ' + endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': 'ml.m5.xlarge',\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'ModelName'   : multi_model_name,\n",
    "        'VariantName' : 'AllTraffic'}])\n",
    "\n",
    "endpoint_name = multi_model_name\n",
    "print('Endpoint name: ' + endpoint_name)\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print('Endpoint Arn: ' + create_endpoint_response['EndpointArn'])\n",
    "\n",
    "print('Waiting for {} endpoint to be in service...'.format(endpoint_name))\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "smrt = boto3.client('sagemaker-runtime')\n",
    "\n",
    "#endpoint_name = 'multimodel-endpoints-2020-06-05-07-55-36'\n",
    "def predict_one_house_value(features, model_name):\n",
    "    print('Using model {} to predict price of this house: {}'.format(model_name,\n",
    "                                                                     features))\n",
    "\n",
    "    _float_features = [float(i) for i in features]\n",
    "    _body = ','.join(map(str, _float_features)) + '\\n'\n",
    "    \n",
    "    _start_time = time.time()\n",
    "\n",
    "    _response = smrt.invoke_endpoint(\n",
    "                        EndpointName=endpoint_name,\n",
    "                        ContentType='text/csv',\n",
    "                        TargetModel=model_name,\n",
    "                        Body=_body)\n",
    "    _predicted_value = json.loads(_response['Body'].read())[0]\n",
    "\n",
    "    _duration = time.time() - _start_time\n",
    "    \n",
    "    print('${:,.2f}, took {:,d} ms\\n'.format(_predicted_value, int(_duration * 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import matplotlib.pyplot as plt\n",
    "# iterate through invocations with random inputs against a random model showing results and latency\n",
    "import numpy as np\n",
    "for i in range(10):\n",
    "    full_model_name = '{}.tar.gz'.format('100SDW_PAT_2001_RF')\n",
    "    out_arr = np.random.randint(low = 0, high = 1099, size = 4799)\n",
    "    predict_one_house_value(out_arr, full_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '407e4f19-5074-4512-a246-975dd182e214',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '407e4f19-5074-4512-a246-975dd182e214',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Mon, 08 Jun 2020 05:08:35 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
