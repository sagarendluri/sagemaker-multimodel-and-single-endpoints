{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHM_NAME = 'multi-modelrf-sagar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon   12.8kB\r",
      "\r\n",
      "Step 1/13 : FROM ubuntu:16.04\n",
      " ---> 005d2078bdfa\n",
      "Step 2/13 : LABEL com.amazonaws.sagemaker.capabilities.multi-models=true\n",
      " ---> Using cache\n",
      " ---> f3c0e6cdd684\n",
      "Step 3/13 : LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true\n",
      " ---> Using cache\n",
      " ---> 078858966e26\n",
      "Step 4/13 : RUN apt-get update &&     apt-get -y install --no-install-recommends     build-essential     ca-certificates     openjdk-8-jdk-headless     python3-dev     curl     vim     && rm -rf /var/lib/apt/lists/*     && curl -O https://bootstrap.pypa.io/get-pip.py     && python3 get-pip.py\n",
      " ---> Using cache\n",
      " ---> 60771f033d5b\n",
      "Step 5/13 : RUN update-alternatives --install /usr/bin/python python /usr/bin/python3 1\n",
      " ---> Using cache\n",
      " ---> aef26bbbd902\n",
      "Step 6/13 : RUN update-alternatives --install /usr/local/bin/pip pip /usr/local/bin/pip3 1\n",
      " ---> Using cache\n",
      " ---> decdd3ebea72\n",
      "Step 7/13 : RUN pip3 --no-cache-dir install mxnet                                 multi-model-server                                 sagemaker-inference                                 retrying\n",
      " ---> Using cache\n",
      " ---> 173693cb91f4\n",
      "Step 8/13 : COPY dockerd-entrypoint.py /usr/local/bin/dockerd-entrypoint.py\n",
      " ---> ce047ca79962\n",
      "Step 9/13 : RUN chmod +x /usr/local/bin/dockerd-entrypoint.py\n",
      " ---> Running in 24604beb1e38\n",
      "Removing intermediate container 24604beb1e38\n",
      " ---> dcfa8a7de828\n",
      "Step 10/13 : RUN mkdir -p /home/model-server/\n",
      " ---> Running in 617c45a9f499\n",
      "Removing intermediate container 617c45a9f499\n",
      " ---> e4877701114c\n",
      "Step 11/13 : COPY model_handler.py /home/model-server/model_handler.py\n",
      " ---> 2b7df6bedf9f\n",
      "Step 12/13 : ENTRYPOINT [\"python\", \"/usr/local/bin/dockerd-entrypoint.py\"]\n",
      " ---> Running in e5fe7482986c\n",
      "Removing intermediate container e5fe7482986c\n",
      " ---> 2e8d53f7978b\n",
      "Step 13/13 : CMD [\"serve\"]\n",
      " ---> Running in 8ced529f8510\n",
      "Removing intermediate container 8ced529f8510\n",
      " ---> a4fd2529dea0\n",
      "Successfully built a4fd2529dea0\n",
      "Successfully tagged multi-model-random-forest-new:latest\n",
      "The push refers to repository [580246529711.dkr.ecr.ap-south-1.amazonaws.com/multi-model-random-forest-new]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Error parsing reference: \"580246529711.dkr.ecr.ap-south-1.amazonaws.com/multi-model-random-forest-new#580246529711.dkr.ecr.ap-south-1.amazonaws.com/multi-model-random-forest-new\" is not a valid repository/tag: invalid reference format\n",
      "An image does not exist locally with the tag: 580246529711.dkr.ecr.ap-south-1.amazonaws.com/multi-model-random-forest-new\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=multi-model-random-forest-new\n",
    "\n",
    "cd container\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "#region=$(aws configure get region)\n",
    "region=${region:-ap-south-1}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "# aws sagemaker list-models\n",
    "\n",
    "#docker image ls\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}#${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Train multiple house value prediction models\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer\n",
    "import boto3\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_hous():\n",
    "    bucket_name = 'sagemaker-data-icr'\n",
    "    s3 = boto3.resource('s3')\n",
    "    data_key = 'first phenomes and genomes.csv'\n",
    "    data_location = 's3://{}/{}'.format(bucket_name, data_key)\n",
    "    df = pd.read_csv(data_location)\n",
    "    #data = df.drop(['Unnamed: 0'],axis=1)\n",
    "    df1 = df.fillna(df.mean())\n",
    "    COLUMNS = list(df1.columns)\n",
    "    \n",
    "    i = str(input(\"enter a feature name \"))\n",
    "    #for i in col:\n",
    "    return i,df1\n",
    "\n",
    "\n",
    "\n",
    "sm_client = boto3.client(service_name='sagemaker')\n",
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime')\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "ACCOUNT_ID  = boto3.client('sts').get_caller_identity()['Account']\n",
    "REGION      = boto3.Session().region_name\n",
    "BUCKET      = sagemaker_session.default_bucket()\n",
    "SCRIPT_FILENAME     = 'train_prediction.py'\n",
    "USER_CODE_ARTIFACTS = 'gp_code2.tar.gz'\n",
    "\n",
    "\n",
    "\n",
    "DATA_PREFIX            = 'gp_rf2'\n",
    "END_MODEL_NAME     = 'MXNET-endpoints'\n",
    "MULTI_MODEL_ARTIFACTS  = 'multi_model_artifacts'\n",
    "\n",
    "TRAIN_INSTANCE_TYPE    = 'ml.m4.xlarge'\n",
    "ENDPOINT_INSTANCE_TYPE = 'ml.m5.xlarge'\n",
    "#Split a given dataset into train, validation, and test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "def launch_training_job(i):\n",
    "    # clear out old versions of the data\n",
    "   # _s3_bucket = s3.Bucket(BUCKET)\n",
    "   # _full_input_prefix = '{}/model_prep/{}'.format(DATA_PREFIX, location)\n",
    "   # _s3_bucket.objects.filter(Prefix=_full_input_prefix + '/').delete()\n",
    "\n",
    "    # upload the entire set of data for all three channels\n",
    "    #_local_folder = 'data/{}'.format(location)\n",
    "    inputs = sagemaker_session.upload_data('data')\n",
    "    print('Training data uploaded: {}'.format(inputs))\n",
    "    \n",
    "    _job = 'gp-{}'.format(i.replace('_', '-'))\n",
    "    _full_output_prefix = '{}/model_artifacts/{}'.format(DATA_PREFIX, \n",
    "                                                        i)\n",
    "    _s3_output_path = 's3://{}/{}'.format(BUCKET, _full_output_prefix)\n",
    "    return _s3_output_path,_job\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9ef93c5f6d35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mLOCATIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mPARALLEL_TRAINING_JOBS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mi\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_hous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m#df1= train_validate_test_split(df1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#save_data_locally(loc,df1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-b495335bbcd4>\u001b[0m in \u001b[0;36mgen_hous\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mCOLUMNS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"enter a feature name \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m#for i in col:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "LOCATIONS  = ['100SDW_EIAR_2008_RF','100SDW_EIAR_2009_RF','100SDW_EU_2008_RF','100SDW_EU_2009_RF']\n",
    "PARALLEL_TRAINING_JOBS = 1\n",
    "training_jobs = []\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "for loc in LOCATIONS[:PARALLEL_TRAINING_JOBS]:\n",
    "    i ,df1=gen_hous() \n",
    "    #df1= train_validate_test_split(df1)\n",
    "    #save_data_locally(loc,df1)\n",
    "   # _s3_output_path,_job = launch_training_job(loc)\n",
    "    # clear out old versions of the data\n",
    "    \n",
    "    _s3_output_path,_job = launch_training_job(i)\n",
    "    _estimator = SKLearn(\n",
    "         entry_point='train_prediction.py', role=role,\n",
    "         train_instance_count=1, train_instance_type=TRAIN_INSTANCE_TYPE,\n",
    "         framework_version='0.20.0',\n",
    "         output_path=_s3_output_path,\n",
    "         base_job_name=_job)\n",
    "    \n",
    "    DISTRIBUTION_MODE = 'FullyReplicated'\n",
    "    \n",
    "    train_input = sagemaker_session.upload_data(\"data\")\n",
    "    _remote_inputs = {'train': train_input}\n",
    "\n",
    "    _estimator.fit(_remote_inputs, wait=False)\n",
    "    training_jobs.append( _estimator.latest_training_job.name)\n",
    "print('{} training jobs launched: {}'.format(len(training_jobs), training_jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_training_job_to_complete(job_name):\n",
    "    print('Waiting for job {} to complete...'.format(job_name))\n",
    "    _resp   = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "    _status = _resp['TrainingJobStatus']\n",
    "    while _status=='InProgress':\n",
    "        time.sleep(60)\n",
    "        _resp   = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "        _status = _resp['TrainingJobStatus']\n",
    "        if _status == 'InProgress':\n",
    "            print('{} job status: {}'.format(job_name, _status))\n",
    "    print('DONE. Status for {} is {}\\n'.format(job_name, _status))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_model_artifacts(model_data_url):\n",
    "    # extract the s3 key from the full url to the model artifacts\n",
    "    _s3_key = model_data_url.split('s3://{}/'.format(BUCKET))[1]\n",
    "    # get the part of the key that identifies the model within the model artifacts folder\n",
    "    _model_name_plus = _s3_key[_s3_key.find('model_artifacts') + len('model_artifacts') + 1:]\n",
    "    # finally, get the unique model name (e.g., \"NewYork_NY\")\n",
    "    _model_name = re.findall('^(.*?)/', _model_name_plus)[0]\n",
    "    return _s3_key, _model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make a copy of the model artifacts from the original output of the training job to the place in\n",
    "# s3 where the multi model endpoint will dynamically load individual models\n",
    "def deploy_artifacts_to_gp(job_name):\n",
    "    _resp = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "    _source_s3_key, _model_name = parse_model_artifacts(_resp['ModelArtifacts']['S3ModelArtifacts'])\n",
    "    _copy_source = {'Bucket': BUCKET, 'Key': _source_s3_key}\n",
    "    _key = '{}/{}/{}.tar.gz'.format(DATA_PREFIX, MULTI_MODEL_ARTIFACTS, _model_name)\n",
    "    \n",
    "    print('Copying {} model\\n   from: {}\\n     to: {}...'.format(_model_name, _source_s3_key, _key))\n",
    "    s3_client.copy_object(Bucket=BUCKET, CopySource=_copy_source, Key=_key)\n",
    "    return _key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job gp-APB-PAT-2001-RF2-2020-06-02-06-03-48-391 to complete...\n",
      "gp-APB-PAT-2001-RF2-2020-06-02-06-03-48-391 job status: InProgress\n",
      "gp-APB-PAT-2001-RF2-2020-06-02-06-03-48-391 job status: InProgress\n",
      "gp-APB-PAT-2001-RF2-2020-06-02-06-03-48-391 job status: InProgress\n",
      "DONE. Status for gp-APB-PAT-2001-RF2-2020-06-02-06-03-48-391 is Completed\n",
      "\n",
      "Copying APB_PAT_2001_RF2 model\n",
      "   from: gp_rf2/model_artifacts/APB_PAT_2001_RF2/gp-APB-PAT-2001-RF2-2020-06-02-06-03-48-391/output/model.tar.gz\n",
      "     to: gp_rf2/multi_model_artifacts/APB_PAT_2001_RF2.tar.gz...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# copy every model except the first one\n",
    "for job in training_jobs:\n",
    "    wait_for_training_job_to_complete(job)\n",
    "    deploy_artifacts_to_gp(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# When using multi-model endpoints with the Scikit Learn container, we need to provide an entry point for\n",
    "# inference that will at least load the saved model. This function uploads a model artifact containing such a\n",
    "# script. This tar.gz file will be fed to the SageMaker multi-model creation and pointed to by the \n",
    "# SAGEMAKER_SUBMIT_DIRECTORY environment variable.\n",
    "\n",
    "def upload_inference_code(script_file_name, prefix):\n",
    "    _tmp_folder = 'inference-code'\n",
    "    if not os.path.exists(_tmp_folder):\n",
    "        os.makedirs(_tmp_folder)\n",
    "    !tar -czvf $_tmp_folder/$USER_CODE_ARTIFACTS $script_file_name > /dev/null\n",
    "    _loc = sagemaker_session.upload_data(_tmp_folder, \n",
    "                                         key_prefix='{}/{}'.format(prefix, _tmp_folder))\n",
    "    return _loc + '/' + USER_CODE_ARTIFACTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_model_entity(multi_model_name, role):\n",
    "    # establish the place in S3 from which the endpoint will pull individual models\n",
    "    _model_url  = 's3://{}/{}/{}/'.format(BUCKET, DATA_PREFIX, MULTI_MODEL_ARTIFACTS)\n",
    "    _container = {\n",
    "        'Image':        MULTI_MODEL_SKLEARN_IMAGE,\n",
    "        'ModelDataUrl': _model_url,\n",
    "        'Mode':         'MultiModel',\n",
    "        'Environment': {\n",
    "            'SAGEMAKER_PROGRAM' : SCRIPT_FILENAME,\n",
    "            'SAGEMAKER_SUBMIT_DIRECTORY' : upload_inference_code(SCRIPT_FILENAME, DATA_PREFIX)\n",
    "        }\n",
    "    }\n",
    "    create_model_response = sm_client.create_model(\n",
    "        ModelName = multi_model_name,\n",
    "        ExecutionRoleArn = role,\n",
    "        Containers = [_container])\n",
    "    \n",
    "    return _model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi model name: MXNET-endpoints-2020-06-08-05-07-42\n",
      "Here are the models that the endpoint has at its disposal:\n",
      "2020-06-02 06:02:52   33.7 KiB 100SDW_PAT_2001_RF.tar.gz\n",
      "2020-06-02 06:07:55   32.0 KiB APB_PAT_2001_RF2.tar.gz\n",
      "\n",
      "Total Objects: 2\n",
      "   Total Size: 65.7 KiB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "multi_model_name = '{}-{}'.format(HOUSING_MODEL_NAME, strftime('%Y-%m-%d-%H-%M-%S', gmtime()))\n",
    "model_url = create_multi_model_entity(multi_model_name, role)\n",
    "print('Multi model name: {}'.format(multi_model_name))\n",
    "\n",
    "print('Here are the models that the endpoint has at its disposal:')\n",
    "!aws s3 ls --human-readable --summarize $model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint config name: MXNET-endpoints-2020-06-08-05-07-42\n",
      "Endpoint name: MXNET-endpoints-2020-06-08-05-07-42\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = multi_model_name\n",
    "print('Endpoint config name: ' + endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': 'ml.m5.xlarge',\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'ModelName'   : multi_model_name,\n",
    "        'VariantName' : 'AllTraffic'}])\n",
    "\n",
    "endpoint_name = multi_model_name\n",
    "print('Endpoint name: ' + endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint config name: MXNET-endpoints-2020-06-08-05-07-42\n",
      "Endpoint name: MXNET-endpoints-2020-06-08-05-07-42\n",
      "Endpoint Arn: arn:aws:sagemaker:ap-south-1:580246529711:endpoint/mxnet-endpoints-2020-06-08-05-07-42\n",
      "Waiting for MXNET-endpoints-2020-06-08-05-07-42 endpoint to be in service...\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = multi_model_name\n",
    "print('Endpoint config name: ' + endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': 'ml.m5.xlarge',\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'ModelName'   : multi_model_name,\n",
    "        'VariantName' : 'AllTraffic'}])\n",
    "\n",
    "endpoint_name = multi_model_name\n",
    "print('Endpoint name: ' + endpoint_name)\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print('Endpoint Arn: ' + create_endpoint_response['EndpointArn'])\n",
    "\n",
    "print('Waiting for {} endpoint to be in service...'.format(endpoint_name))\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "smrt = boto3.client('sagemaker-runtime')\n",
    "\n",
    "#endpoint_name = 'multimodel-endpoints-2020-06-05-07-55-36'\n",
    "def predict_one_house_value(features, model_name):\n",
    "    print('Using model {} to predict price of this house: {}'.format(model_name,\n",
    "                                                                     features))\n",
    "\n",
    "    _float_features = [float(i) for i in features]\n",
    "    _body = ','.join(map(str, _float_features)) + '\\n'\n",
    "    \n",
    "    _start_time = time.time()\n",
    "\n",
    "    _response = smrt.invoke_endpoint(\n",
    "                        EndpointName=endpoint_name,\n",
    "                        ContentType='text/csv',\n",
    "                        TargetModel=model_name,\n",
    "                        Body=_body)\n",
    "    _predicted_value = json.loads(_response['Body'].read())[0]\n",
    "\n",
    "    _duration = time.time() - _start_time\n",
    "    \n",
    "    print('${:,.2f}, took {:,d} ms\\n'.format(_predicted_value, int(_duration * 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import matplotlib.pyplot as plt\n",
    "# iterate through invocations with random inputs against a random model showing results and latency\n",
    "import numpy as np\n",
    "for i in range(10):\n",
    "    full_model_name = '{}.tar.gz'.format('100SDW_PAT_2001_RF')\n",
    "    out_arr = np.random.randint(low = 0, high = 1099, size = 4799)\n",
    "    predict_one_house_value(out_arr, full_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '407e4f19-5074-4512-a246-975dd182e214',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '407e4f19-5074-4512-a246-975dd182e214',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Mon, 08 Jun 2020 05:08:35 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
